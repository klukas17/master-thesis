@Inbook{nocedal2006numericaloptimization,
	author = {Nocedal, Jorge and Wrigth, Stephen J.},
	title = {Numerical Optimization, 2nd edition},
	pages = "2--7",
	publisher = {Springer},
	year = {2006}
}

@Inbook{sipser2012computation,
	author = {Sipser, Michael},
	title = {Introduction to the Theory of Computation, 3rd edition},
	pages = "292--322",
	publisher = {Cengage Learning},
	year = {2012}
}

@Inbook{swamy2016search,
  author = {Du, Ke-Lin and Swamy, M.N.S.},
  title = {Search and Optimization by Metaheuristics},
  pages = "9--11",
  publisher = {Springer},
  year = {2016}
}

@Inbook{Chakhlevitch2008,
	author="Chakhlevitch, Konstantin and Cowling, Peter",
	editor="Cotta, Carlos and Sevaux, Marc and S{\"o}rensen, Kenneth",
	title="Hyperheuristics: Recent Developments",
	bookTitle="Adaptive and Multilevel Metaheuristics",
	year="2008",
	publisher="Springer Berlin Heidelberg",
	address="Berlin, Heidelberg",
	pages="3--29",
	abstract="Given their economic importance, there is continuing research interest in providing better and better solutions to real-world scheduling problems. The models for such problems are increasingly complex and exhaustive search for optimal solutions is usually impractical. Moreover, difficulty in accurately modelling the problems means that mathematically ``optimal'' solutions may not actually be the best possible solutions in practice. Therefore heuristic methods are often used, which do not guarantee optimal or even near optimal solutions. The main goal of heuristics is to produce solutions of acceptable quality in reasonable time. The problem owners often prefer simple, easy to implement heuristic approaches which do not require significant amount of resources for their development and implementation [12]. However, such individual heuristics do not always perform well for the variety of problem instances which may be encountered in practice. There is a wide range of modern heuristics known from the literature which are specifically designed and tuned to solve certain classes of optimisation problems. These methods are based on the partial search of the solution space and often referred as metaheuristics.",
	isbn="978-3-540-79438-7",
	doi="10.1007/978-3-540-79438-7_1",
	url="https://doi.org/10.1007/978-3-540-79438-7_1"
}

@Inbook{pinedo2016scheduling,
  author         = {Pinedo, Michael L.},
  publisher      = {Springer},
  title          = {Scheduling: Theory, Algorithms and Systems, 5th edition},
  pages          = {1--33},
  year           = {2016}
}

@article{werner2013survey,
  author         = {Werner, Frank},
  editor         = {Patrick Siarry},
  publisher      = {Nova Science Publisher},
  title          = {A Survey of Genetic Algorithms for Shop Scheduling Problems},
  year           = {2013}
}

@misc{lekin,
  author       = {Pinedo, Michael L. and Chao, Xiuli and Leung Joseph},
  title        = {Lekin},
  url          = {https://web-static.stern.nyu.edu/om/software/lekin/},
  note         = {Accessed: 9th June 2024}
}

@misc{drawio,
  title        = {Flowchart Maker and Online Diagram Software},
  url          = {https://app.diagrams.net/},
  note         = {Accessed: June 2024}
}

@article{grasp,
  author          = {Resende, Mauricio G.C.},
  journal         = {Journal of Global Optimization},
  title           = {Greedy Randomized Adaptive Search Procedures},
  year            = {1995}
}

@article{random_search,
  author          = {Bergstra, James and Bengio, Joshua},
  journal         = {Journal of Machine Learning Research},
  title           = {Random Search for Hyper-Parameter Optimization},
  volume          = {13},
  year            = {2012}
}

@article{variable_neighborhood_search,
  author          = {MladenoviÄ‡, N. and Hansen, P.},
  journal         = {Computers \& Operations Research},
  title           = {Variable neighborhood search},
  volume          = {24},
  year            = {1997}
}

@Inbook{iterated_local_search,
author="Louren{\c{c}}o, Helena R.
and Martin, Olivier C.
and St{\"u}tzle, Thomas",
editor="Gendreau, Michel
and Potvin, Jean-Yves",
title="Iterated Local Search: Framework and Applications",
bookTitle="Handbook of Metaheuristics",
year="2010",
publisher="Springer US",
address="Boston, MA",
pages="363--397",
abstract="The key idea underlying iterated local search is to focus the search not on the full space of all candidate solutions but on the solutions that are returned by some underlying algorithm, typically a local search heuristic. The resulting search behavior can be characterized as iteratively building a chain of solutions of this embedded algorithm. The result is also a conceptually simple metaheuristic that nevertheless has led to state-of-the-art algorithms for many computationally hard problems. In fact, very good performance is often already obtained by rather straightforward implementations of the metaheuristic. In addition, the modular architecture of iterated local search makes it very suitable for an algorithm engineering approach where, progressively, the algorithms' performance can be further optimized. Our purpose here is to give an accessible description of the underlying principles of iterated local search and a discussion of the main aspects that need to be taken into account for a successful application of it. In addition, we review the most important applications of this method and discuss its relationship to other metaheuristics.",
isbn="978-1-4419-1665-5",
doi="10.1007/978-1-4419-1665-5_12",
url="https://doi.org/10.1007/978-1-4419-1665-5_12"
}

@inbook{artificial_intelligence,
  author         = {Russel, Stuart J. and Norvig, Peter},
  pages          = {122-125},
  publisher      = {Pearson Education, Inc.},
  title          = {Artificial Intelligence: A Modern Approach, 3rd edition},
  year           = {2010}
}

@inproceedings{ssga,
author = {Syswerda, Gilbert},
year = {1989},
month = {01},
title = {Uniform Crossover in Genetic Algorithms},
journal = {Proc. 3rd Intl Conference on Genetic Algorithms 1989}
}

@article{clonalg,
author = {De Castro, Leandro and Von Zuben, Fernando},
year = {2002},
month = {07},
pages = {239 - 251},
title = {Learning and Optimization Using the Clonal Selection Principle},
volume = {6},
journal = {Evolutionary Computation, IEEE Transactions on},
doi = {10.1109/TEVC.2002.1011539}
}

@InProceedings{sia,
author="Cutello, Vincenzo
and Nicosia, Giuseppe",
editor="Garijo, Francisco J.
and Riquelme, Jos{\'e} C.
and Toro, Miguel",
title="An Immunological Approach to Combinatorial Optimization Problems",
booktitle="Advances in Artificial Intelligence --- IBERAMIA 2002",
year="2002",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="361--370",
abstract="In this work we use a simplified model of the immune system to explore the problem solving feature. We consider only two immunological entities, antigens and antibodies, two parameters, and simple immune operators. The experimental results shows how a simple randomized search algorithm coupled with a mechanism for adaptive recognition of hardest constraints, is sufficient to obtain optimal solutions for any combinatorial optimization problem.",
isbn="978-3-540-36131-2"
}

@article{es,
author = {Beyer, Hans-Georg and Schwefel, Hans-Paul},
year = {2002},
month = {03},
pages = {3-52},
title = {Evolution strategies - A comprehensive introduction},
volume = {1},
journal = {Natural Computing},
doi = {10.1023/A:1015059928466}
}

@book{gga,
  address = {New York},
  author = {Goldberg, David E.},
  biburl = {https://www.bibsonomy.org/bibtex/27ae824509ef87f5f6d333d700254364c/dalbem},
  publisher = {Addison-Wesley},
  title = {Genetic Algorithms in Search, Optimization, and Machine Learning},
  username = {dalbem},
  year = {1989},
  pages = {1--25}
}

@inproceedings{cicirello2023ecta,
AUTHOR = {Vincent A. Cicirello},
TITLE = {A Survey and Analysis of Evolutionary Operators for Permutations},
BOOKTITLE = {Proceedings of the 15th International Joint Conference on Computational Intelligence},
DOI = {10.5220/0012204900003595},
MONTH = {November},
PAGES = {288--299},
YEAR = {2023},
URL = {https://www.cicirello.org/publications/cicirello2023ecta.pdf}
}

@Inbook{nn,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016},
    pages ={168--227}
}

@Inbook{tbgp,
  added-at = {2008-11-22T15:57:31.000+0100},
  address = {Cambridge, MA, USA},
  author = {Koza, John R.},
  biburl = {https://www.bibsonomy.org/bibtex/27573e564bc5369e1a853e74b3ac62607/emanuel},
  interhash = {e8307fb6cf4ee27405142256d98c4c9e},
  intrahash = {7573e564bc5369e1a853e74b3ac62607},
  isbn = {0-262-11170-5},
  keywords = {enumerative_ip gp induction inductive_programming program_evolution program_synthesis},
  publisher = {MIT Press},
  timestamp = {2008-11-22T15:57:31.000+0100},
  title = {Genetic Programming: {O}n the Programming of Computers by Means of Natural Selection},
  year = 1992,
  pages = {73--120}
}

@Inbook{cgp,
author="Miller, Julian F.",
editor="Miller, Julian F.",
title="Cartesian Genetic Programming",
bookTitle="Cartesian Genetic Programming",
year="2011",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="17--34",
abstract="In this chapter, we describe the original and most widely known form of Cartesian genetic programming (CGP). CGP encodes computational structures, which we call `programs' in the form of directed acyclic graphs. We refer to this as `classic' CGP. However these program may be computer programs, circuits, rules, or other specialized computational entities.",
isbn="978-3-642-17310-3",
doi="10.1007/978-3-642-17310-3_2",
url="https://doi.org/10.1007/978-3-642-17310-3_2"
}

@Inbook{lgp,
  title = {{Linear Genetic Programming}},
  publisher = {Springer},
  year = {2007},
  author = {Brameier, M.F. and Banzhaf, W.},
  address = {New York},
  pages = {13--34}
}

@article{gep,
  author       = {C{\^{a}}ndida Ferreira},
  title        = {Gene Expression Programming: a New Adaptive Algorithm for Solving
                  Problems},
  journal      = {CoRR},
  volume       = {cs.AI/0102027},
  year         = {2001},
  url          = {https://arxiv.org/abs/cs/0102027},
  timestamp    = {Fri, 10 Jan 2020 12:58:57 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/cs-AI-0102027.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{mep,
author = {Oltean, Mihai and Dumitrescu, Dan},
year = {2002},
month = {01},
pages = {},
title = {Multi Expression Programming}
}

@INPROCEEDINGS{sbgp,
  author={Perkis, T.},
  booktitle={Proceedings of the First IEEE Conference on Evolutionary Computation. IEEE World Congress on Computational Intelligence}, 
  title={Stack-based genetic programming}, 
  year={1994},
  volume={},
  number={},
  pages={148-153 vol.1},
  keywords={Genetic programming;Virtual machining;Genetic engineering;Shape;Tagging;Assembly;Protection},
  doi={10.1109/ICEC.1994.350025}}

@Inbook{ge,
author="O'Neil, Michael
and Ryan, Conor",
title="Grammatical Evolution",
bookTitle="Grammatical Evolution: Evolutionary Automatic Programming in an Arbitrary Language",
year="2003",
publisher="Springer US",
address="Boston, MA",
pages="33--47",
abstract="This chapter describes Grammatical Evolution (GE) in detail (Ryan et al., 1998; O'Neill and Ryan, 2001; O'Neill, 2001). We show that it is an evolutionary algorithm (EA) that can evolve complete programs in an arbitrary language using a variable-length binary string. The binary genome determines which production rules in a Backus Naur Form (BNF) grammar definition are used in a genotype-to-phenotype mapping process to a program. GE is set up such that the evolutionary algorithm is independent of the output programs by virtue of the genotype-phenotype mapping, allowing GE to take advantage of advances in EA research. The BNF grammar, like the EA, is a plug-in component of the system that determines the syntax and language of the output code, hence, it is possible to evolve programs in an arbitrary language.",
isbn="978-1-4615-0447-4",
doi="10.1007/978-1-4615-0447-4_4",
url="https://doi.org/10.1007/978-1-4615-0447-4_4"
}

@Article{sge,
author={Louren{\c{c}}o, Nuno
and Pereira, Francisco B.
and Costa, Ernesto},
title={Unveiling the properties of structured grammatical evolution},
journal={Genetic Programming and Evolvable Machines},
year={2016},
month={Sep},
day={01},
volume={17},
number={3},
pages={251-289},
abstract={Structured grammatical evolution (SGE) is a new genotypic representation for grammatical evolution (GE). It comprises a hierarchical organization of the genes, where each locus is explicitly linked to a non-terminal of the grammar being used. This one-to-one correspondence ensures that the modification of a gene does not affect the derivation options of other non-terminals. We present a comprehensive set of optimization results obtained with problems from three different categories: symbolic regression, path finding, and predictive modeling. In most of the situations SGE outperforms standard GE, confirming the effectiveness of the new representation. To understand the reasons for SGE enhanced performance, we scrutinize its main features. We rely on a set of static measures to model the interactions between the representation and variation operators and assess how they influence the interplay between the genotype-phenotype spaces. The study reveals that the structured organization of SGE promotes an increased locality and is less redundant than standard GE, thus fostering an effective exploration of the search space.},
issn={1573-7632},
doi={10.1007/s10710-015-9262-4},
url={https://doi.org/10.1007/s10710-015-9262-4}
}

@inbook{gbgp,
author = {Atkinson, Timothy and Plump, Detlef and Stepney, Susan},
year = {2018},
month = {03},
pages = {35-51},
title = {Evolving Graphs by Graph Programming},
isbn = {978-3-319-77552-4},
doi = {10.1007/978-3-319-77553-1_3}
}

@Inbook{gbgp_operators,
author="Machado, Penousal
and Correia, Jo{\~a}o
and Assun{\c{c}}{\~a}o, Filipe",
editor="Gandomi, Amir H.
and Alavi, Amir H.
and Ryan, Conor",
title="Graph-Based Evolutionary Art",
bookTitle="Handbook of Genetic Programming Applications",
year="2015",
publisher="Springer International Publishing",
address="Cham",
pages="3--36",
abstract="A graph-based approach for the evolution of Context Free Design Grammars is presented. Each genotype is a directed hierarchical graph and, as such, the evolutionary engine employs graph-based crossover and mutation. We introduce six different fitness functions based on evolutionary art literature and conduct a wide set of experiments. We begin by assessing the adequacy of the system and establishing the experimental parameters. Afterwards, we conduct evolutionary runs using each fitness function individually. Finally, experiments where a combination of these functions is used to assign fitness are performed. Overall, the experimental results show the ability of the system to optimize the considered functions, individually and combined, and to evolve images that have the desired visual characteristics.",
isbn="978-3-319-20883-1",
doi="10.1007/978-3-319-20883-1_1",
url="https://doi.org/10.1007/978-3-319-20883-1_1"
}

@Article{matplotlib,
  Author    = {Hunter, J. D.},
  Title     = {Matplotlib: A 2D graphics environment},
  Journal   = {Computing in Science \& Engineering},
  Volume    = {9},
  Number    = {3},
  Pages     = {90--95},
  abstract  = {Matplotlib is a 2D graphics package used for Python for
  application development, interactive scripting, and publication-quality
  image generation across user interfaces and operating systems.},
  publisher = {IEEE COMPUTER SOC},
  doi       = {10.1109/MCSE.2007.55},
  year      = 2007
}

@Article{hyperopti,
author={Shanthi, D. L.
and Chethan, N.},
title={Genetic Algorithm Based Hyper-Parameter Tuning to Improve the Performance of Machine Learning Models},
journal={SN Computer Science},
year={2022},
month={Dec},
day={22},
volume={4},
number={2},
pages={119},
abstract={Parameter setting will have a great impact on overall behavior of a machine learning model in terms of training time, infrastructure resource requirements, model convergence, and model accuracy. While training machine learning models, it is very difficult to choose optimum values for various parameters to create the final model architecture. There are two types of parameters in machine learning model, one is referred as model parameters that are estimated by fitting the given data to the model. And the other is referred as model hyperparameters, these parameters are used to control the learning process. Model parameters are determined by machine ideally by exploration and automatically picks the optimum value; for example, the weights given to a neural network continuously update throughout each iteration until an optimal value is not reached. The method of hyperparameter tuning aims to determine the optimal combination of hyperparameters that will enable the model to function optimally. Setting the optimal mix of hyperparameters is the only method to maximize model performance. However, the designer is responsible for setting the hyperparameters that define the model architecture, such as the value of k in a kNN model, and the process of finding the optimum hyperparameter is referred to as hyperparameter tuning. Currently, this is handled in a variety of methods, including random searching of a specific solution space, sequential searching of the solution space using grids, and so on. In this article, comparative analysis of these methods to the genetic algorithm methodology for hyperparameter tuning is tested.},
issn={2661-8907},
doi={10.1007/s42979-022-01537-8},
url={https://doi.org/10.1007/s42979-022-01537-8}
}

@InBook{walpole,
  address = {Upper Saddle River},
  author = {Walpole, Ronald E. and Myers, Raymond H. and Myers, Sharon L. and Ye, Keying},
  edition = {9th},
  publisher = {Pearson Education},
  title = {Probability \& statistics for engineers and scientists},
  year = 2012,
  pages = {336--342}
}
